{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification using multiple classifiers\n",
    "Team Members: Lisa Korntheuer, Jan Birkert, Adrian Desiderato, Jan Wangerin, Spyridon Spyropoulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data understanding\n",
    "Data describe (Features, Target etc.)\n",
    "- filename and length irrelevant for ML\n",
    "- 57 features -> PCA?\n",
    "- only numerical data except for class labels (\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/features_30_sec.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df.iloc[:, 2:-2].corr()\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(cor, square = True, xticklabels=True, yticklabels=True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are quite a few feature combinations with high correlations, PCA may be worth a try. (See Data Prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "Jan W.\n",
    "\n",
    "Data splitting\n",
    "\n",
    "y = LabelEncoder() \n",
    "\n",
    "MinMax()\n",
    "Das andere() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelEnc = LabelEncoder()\n",
    "y = df['label']\n",
    "y = pd.DataFrame(LabelEnc.fit_transform(y))\n",
    "df['label_enc'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mms = MinMaxScaler()\n",
    "scaler_ss = StandardScaler()\n",
    "X = df.loc[:, 'chroma_stft_mean' : 'mfcc20_var']\n",
    "X_scaled_array_mms = scaler_mms.fit_transform(X)\n",
    "X_scaled_array_ss = scaler_ss.fit_transform(X)\n",
    "X_scaled_mms = pd.DataFrame(X_scaled_array_mms, columns=X.columns)\n",
    "X_scaled_ss = pd.DataFrame(X_scaled_array_ss, columns=X.columns)\n",
    "print(X)\n",
    "print(X_scaled_mms)\n",
    "print(X_scaled_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA: (copied from Material Notebook 04, probably has to be adjusted later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA() # typically you add here as a parameter the nbr. of cmponents: i.e.: n_components=2\n",
    "            # we leave it blank to get all!\n",
    "pcs = pca.fit_transform(X_scaled_ss) # principle components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Components (Dot Product of Data and Eigenvectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pcs[:5])\n",
    "print()\n",
    "print(len(pcs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scree Plot with Kaiser Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "pc_values = np.arange(pca.n_components_) + 1\n",
    "ax.plot(pc_values, pca.explained_variance_, 'o-', linewidth=2, color='blue')\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.axhline(y=1, linewidth=1, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potentially, a lot of dimensions could be removed according to the Kaiser criteria. The following enumeration shows how much \"information\" is contained in how many of the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [10, 15, 30, 45]:\n",
    "    print(np.sum(pca.explained_variance_ratio_[:i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fight the curse of dimensionality, some dimensions could be removed, for example the last 12 to even 27 dimensions, since about 94% of \"information\" is contained in the first 30 PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_train_mms, X_test_mms, y_train_mms, y_test_mms = train_test_split(X_scaled_mms, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss = train_test_split(X_scaled_ss, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training \n",
    "\n",
    "Each Modell is trained and the quality of the classifier(accuracy) is displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forests\n",
    "Spyridon \n",
    "\n",
    "In this section Random Forest as a classifier will be tested. In the first step all important libraries will be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.tree as tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Simple Hyperparameter tuning \n",
    "When training random forests, there is no heavy hyperparameter needed to get a good accuracy. The structure of the model is already decreasing Bias and Variance by injecting randomness on constructing the trees. By Random Feature selection and bagging the Risk of Overfitting is minimized, and by tuning the hyperparameters, the Underfitting risk is also minimized. So it is enough only to tune the numbers of trees in the ensemlbe \"n_estimators\" and the spliting criterion. All other hyperparameters will use the default values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "param_grid = {'n_estimators': np.array([ 100, 250, 500, 1000, 2000, 7000]), \n",
    "              'criterion':['gini','entropy', 'log_loss'],\n",
    "              }\n",
    "grid_search_rf_simple = GridSearchCV(rf, param_grid, n_jobs=-1, cv=2, scoring='accuracy', verbose=1, refit=True)\n",
    "grid_search_rf_simple.fit(X_train, y_train.values.ravel())\n",
    "y_pred_rf_simple = grid_search_rf_simple.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch found out the best model, the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf_simple.score(X_test, y_test)\n",
    "print(\"Best Score: %f\" % grid_search_rf_simple.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", grid_search_rf_simple.best_params_)\n",
    "print(\"Optimal Model: \", grid_search_rf_simple.best_estimator_)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf_simple)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in ~20 seconds (on my machine), Gridsearch found a model with 76% accuracy. That's a really good result! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Heavy Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But lets also try some heavy hyperparameter tuning to see what results can be achieved: (This takes some time....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_heavy = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "param_grid_rf_heavy = {'n_estimators': np.array([ 100, 250, 500, 1000, 2000, 7000]), \n",
    "              'criterion':['gini','entropy', 'log_loss'],\n",
    "              'max_depth': np.array([3,5, 7,10, None]),\n",
    "                'min_samples_split': np.array([2, 5, 10]),\n",
    "                'min_samples_leaf': np.array([1, 2, 4]),\n",
    "                'max_features': np.array(['sqrt', 'log2'])\n",
    "              }\n",
    "grid_search_rf_heavy = GridSearchCV(rf_heavy, param_grid_rf_heavy, n_jobs=-1, cv=2, scoring='accuracy', verbose=1, refit=True)\n",
    "grid_search_rf_heavy.fit(X_train, y_train.values.ravel())\n",
    "y_pred_heavy = grid_search_rf_heavy.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf_heavy.score(X_test, y_test)\n",
    "print(\"Best Score: %f\" % grid_search_rf_heavy.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", grid_search_rf_heavy.best_params_)\n",
    "print(\"Optimal Model: \", grid_search_rf_heavy.best_estimator_)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_heavy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Best Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with heavy hyperparameter tuning, that needed more than 15 minutes, the resulting forest is not really giving much more performance. There might be a better slightly better score of the found model, but the resulting accuracy of the model is worse. So our best Random Forest model is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_simple_best = grid_search_rf_simple.best_estimator_\n",
    "rf_simple_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the resulting model.\n",
    "\n",
    "We start by looking into the feature importance of the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_simple_best = grid_search_rf_simple.best_estimator_\n",
    "\n",
    "importances = rf_simple_best.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns\n",
    "\n",
    "\n",
    "top_n = 15\n",
    "top_indices = indices[:top_n]\n",
    "top_importances = importances[top_indices]\n",
    "\n",
    "top_feature_names = [feature_names[idx] for idx in top_indices]\n",
    "print(\"Feature ranking with names:\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"Top {top_n} Feature Importances\")\n",
    "plt.bar(range(top_n), top_importances, align=\"center\")\n",
    "plt.xticks(range(top_n), top_feature_names, rotation=45, ha='right')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.xlim([-1, top_n])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Decision trees\n",
    "\n",
    "Jan W.\n",
    "\n",
    "First try using post-pruning and the entire dataset. Post-pruning is done using hyperparameter-tuning with GridsearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn import tree\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0) #maybe use variable for random state so that all classifiers can be adjusted at the same time\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'ccp_alpha':ccp_alphas[:-1].tolist()}\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=0), parameters, cv=10, refit=True)\n",
    "gs.fit(X_train,y_train)\n",
    "tree_best = gs.best_estimator_\n",
    "pred = tree_best.predict(X_test)\n",
    "print('Accuracy', accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rules = export_text(tree_best, feature_names=X.columns)\n",
    "print(rules)\n",
    "print()\n",
    "print(\"Feature importance:\\n\")\n",
    "feature_importance = {}\n",
    "i = 0\n",
    "for col in X.columns:\n",
    "    feature_importance[col] = tree_best.feature_importances_[i]\n",
    "    i += 1\n",
    "features_sorted = sorted(feature_importance.items(), key=lambda x : x[1])\n",
    "features_sorted.reverse()\n",
    "for feature in features_sorted:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "text = tree.plot_tree(tree_best, \n",
    "                   feature_names=X.columns.to_list(), \n",
    "                   filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe try pre pruning with lower maximum height of tree, although that probably won't lead to better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "params = {'max_depth':np.arange(3,15),\n",
    "#          'min_samples_leaf':[3,5,10,15,20],\n",
    "#          'min_samples_split':[8,10,12,18,20,16],\n",
    "          'criterion':['gini','entropy']}\n",
    "gs = GridSearchCV(cls, params, scoring='accuracy', cv=10, verbose=3, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "params_optimal = gs.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % gs.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_best = DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=11) #, min_samples_leaf=20, min_samples_split=8)\n",
    "tree_best.fit(X_train, y_train)\n",
    "pred = tree_best.predict(X_test)\n",
    "\n",
    "print('Test accuracy',accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "text = tree.plot_tree(tree_best, \n",
    "                   feature_names=X.columns.to_list(), \n",
    "                   filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try reduction of dimensions with PCA (only first 30 or so dimensions?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the music genres are classified with the **k-Nearest Neighbors** algorithm. To enhance model performance, it can be useful to tune the following three hyperparameters via cross validation:\n",
    "* ***n_neighbors***  :  number of neighbors $k$\n",
    "* ***weights***  :  weights assigned to the nearest neighbors, especially relevant in case of ties\n",
    "  - 'uniform'  :  all neighbors have equal weights\n",
    "  - 'distance'  :  neighbors closer to the target point have higher weights\n",
    "* ***metric***  :  method for distance computation\n",
    "  - 'euclidean'  :  Euclidean distance\n",
    "  - 'manhatten'  :  Manhatten distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define the possible values for each of these parameters in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter combinations for hyperparameter tuning via cross validation \n",
    "params = {'n_neighbors': np.arange(1,20),               # parameter 'k' \n",
    "              'weights': ['uniform', 'distance'],       # parameter 'weights'\n",
    "              'metric' : ['euclidean','manhattan']}     # parameter 'metric'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then hyperparameter tuning is performed with the help of *GridSearchCV*, using 10-fold cross validation and accuracy as evaluation measure. The model is trained on the training data which have been normalized with the *MinMaxScaler*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "# Use GridSearchCV to tune the chosen parameters\n",
    "gs = GridSearchCV(knn, params, scoring='accuracy', cv=10, verbose=3, n_jobs=-1, refit=True)\n",
    "# Train\n",
    "gs.fit(X_train_mms, y_train_mms.values.ravel())    # Use training data scaled with MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the optimal parameter set, choosing $k=3$ nearest neighbors, distance-dependent weights and Manhattan distance turns out to be the best combination in this experiment. Yet, it must be noted that distance-related weights are also computed if there are no ties, which might lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_optimal = gs.best_params_\n",
    "\n",
    "print(\"Best score: %f\" % gs.best_score_)\n",
    "print(\"Optimal hyperparameters: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this optimal classifier is taken to predict the music genres in the corresponding test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal classifier to predict\n",
    "knn_optimal = gs.best_estimator_\n",
    "y_pred_optimal = knn_optimal.predict(X_test_mms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation shows that the tuned kNN model performs with an accuracy of 74% on these training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for tuned KNN\n",
    "accuracy = accuracy_score(y_test_mms, y_pred_optimal)\n",
    "print('Accuracy:', accuracy)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the resulting best models will be compared. Let's import needed libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we create an array with the best models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [knn_optimal, tree_best, rf_simple_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models: \n",
    "    model.fit(X_train_mms, y_train_mms)\n",
    "    print(model)\n",
    "    y_pred = model.predict(X_test_mms)\n",
    "    accuracy = accuracy_score(y_test_mms, y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision of each model \n",
    "for model in models: \n",
    "    y_pred = model.predict(X_test_mms)\n",
    "    print(model)\n",
    "    precision = precision_score(y_test_mms, y_pred, average='weighted')\n",
    "    print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall of each model\n",
    "\n",
    "for model in models: \n",
    "    y_pred = model.predict(X_test_mms)\n",
    "    print(model)\n",
    "    recall = recall_score(y_test_mms, y_pred, average='weighted')\n",
    "    print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 ROC, AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Binarize the output\n",
    "y_test_bin = label_binarize(y_test_mms, classes=np.unique(y))\n",
    "y_train_bin = label_binarize(y_train_mms, classes=np.unique(y))\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "colors = ['red', 'blue', 'green']\n",
    "linestyles = ['-', '--', '-.']\n",
    "classifiers = models\n",
    "labels = ['KNN', 'Decision Trees', 'Random Forest']\n",
    "\n",
    "for clf, label, clr, ls in zip(classifiers, labels, colors, linestyles):\n",
    "    classifier = OneVsRestClassifier(clf)\n",
    "    y_score = classifier.fit(X_train_mms, y_train_bin).predict_proba(X_test_mms)    \n",
    "    # Compute micro-average ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot the micro-average ROC curve\n",
    "    plt.plot(fpr, tpr, color=clr, linestyle=ls, label='%s (AUC = %0.2f)' % (label, roc_auc))\n",
    "\n",
    "# Add a diagonal line for reference\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=2)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/roc.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# CELL INDEX: 73\n",
    "\n",
    "# Create a table comparing our three main models: KNN, Decision Tree, and Random Forest\n",
    "\n",
    "model_entries = []\n",
    "model_list = [\n",
    "    (\"KNN\", knn_optimal),\n",
    "    (\"Decision Tree\", tree_best),\n",
    "    (\"Random Forest\", rf_simple_best)\n",
    "]\n",
    "\n",
    "for model_name, model_obj in model_list:\n",
    "    # Retrain each model on the (MinMax scaled) training set\n",
    "    model_obj.fit(X_train_mms, y_train_mms)\n",
    "    \n",
    "    # Predict on the (MinMax scaled) test set\n",
    "    y_pred = model_obj.predict(X_test_mms)\n",
    "    \n",
    "    # Calculate standard classification metrics\n",
    "    acc = accuracy_score(y_test_mms, y_pred)\n",
    "    prec = precision_score(y_test_mms, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test_mms, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test_mms, y_pred, average='weighted')\n",
    "    \n",
    "    # For ROC AUC in multi-class, do a OneVsRest scheme (micro-average)\n",
    "    classifier_ovr = OneVsRestClassifier(model_obj)\n",
    "    y_score = classifier_ovr.fit(X_train_mms, y_train_bin).predict_proba(X_test_mms)\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "    \n",
    "    # Collect metrics into a dictionary\n",
    "    model_entries.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision (weighted)\": prec,\n",
    "        \"Recall (weighted)\": rec,\n",
    "        \"F1 Score (weighted)\": f1,\n",
    "        \"ROC AUC (micro)\": roc_auc_val\n",
    "    })\n",
    "\n",
    "# Create and display the comparison table\n",
    "comparison_df = pd.DataFrame(model_entries)\n",
    "comparison_df.style.format(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Algorithm**       | **Description**                                                                 | **Advantages**                                                                                     | **Disadvantages**                                                                                  | **Use Cases**                                                                                   |\n",
    "|---------------------|---------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| **Random Forests**  | An ensemble method that combines multiple decision trees to improve performance | - High accuracy<br>- Robust to overfitting<br>- Handles large datasets well<br>- Feature importance | - Computationally intensive<br>- Less interpretable than single decision trees                    | - Classification and regression<br>- Feature selection<br>- Anomaly detection                   |\n",
    "| **K-Nearest Neighbors (KNN)** | A simple, instance-based learning algorithm that classifies based on the majority vote of nearest neighbors | - Simple to implement<br>- No training phase<br>- Works well with small datasets                   | - Computationally expensive for large datasets<br>- Sensitive to irrelevant features and noise    | - Classification and regression<br>- Recommender systems<br>- Image and pattern recognition     |\n",
    "| **Artificial Neural Networks (ANN)** | A computational model inspired by the human brain, consisting of interconnected nodes (neurons) | - High accuracy for complex problems<br>- Capable of learning non-linear relationships<br>- Versatile | - Requires large amounts of data<br>- Computationally intensive<br>- Difficult to interpret       | - Image and speech recognition<br>- Natural language processing<br>- Time series forecasting    |\n",
    "| **Decision Trees**  | A tree-like model of decisions and their possible consequences                  | - Easy to interpret and visualize<br>- Handles both numerical and categorical data<br>- Non-parametric | - Prone to overfitting<br>- Can be unstable with small variations in data                         | - Classification and regression<br>- Feature selection<br>- Decision analysis and support       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OPTIONAL: Song import and classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
