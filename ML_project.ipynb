{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification using multiple classifiers\n",
    "Team Members: Lisa Korntheuer, Jan Birkert, Adrian Desiderato, Jan Wangerin, Spyridon Spyropoulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music offers a variety of genres, ranging from classical compositions to modern pieces such as jazz songs, rock and pop hits. Although they might overlap to a certain degree, each genre comes with its own sound characteristics. This makes it interesting to find out how songs can be classified into different music genres with the help of machine learning. Recently, only few apps have been developed to categorize the genre of an unknown music piece on the basis of its properties. Instead, popular apps such as _Shazam_ rather identify the title and interpreter of registered songs in a huge database. Therefore, this notebook uses a dataset from Kaggle and compares the performance of four supervised learning algorithms, namely *Random Forest (RF),* *Decision Trees (DT),* *k-Nearest Neighbors (kNN)* and *Artificial Neural Networks (ANN)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that we start by adding imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used can be found in kaggle: https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data understanding\n",
    "\n",
    "The GTZAN Dataset is a widely used collection for music genre classification tasks. The dataset contains 1,000 audio tracks across 10 genres and often various audio features from these tracks are extracted to facilitate analysis and classification. The set we used includes 29 features, which can be categorized as follows:\n",
    "\n",
    "1. Temporal Features:\n",
    "\n",
    "    Zero Crossing Rate: Measures the rate at which the audio signal changes sign, indicating signal noisiness.<br>\n",
    "    Root Mean Square Energy: Represents the signal's energy, correlating with perceived loudness.\n",
    "\n",
    "3. Spectral Features:\n",
    "\n",
    "    Spectral Centroid: Indicates the \"center of mass\" of the spectrum, associated with the brightness of a sound.<br>\n",
    "    Spectral Bandwidth: Measures the width of the spectrum, reflecting the range of frequencies present.<br>\n",
    "    Spectral Contrast: Captures the difference in amplitude between peaks and valleys in the spectrum.<br>\n",
    "    Spectral Flatness: Assesses how flat or peaky a spectrum is, distinguishing tonal from noise-like sounds.<br>\n",
    "    Spectral Rolloff: The frequency below which a certain percentage (typically 85%) of the total spectral energy is contained.\n",
    "\n",
    "4. Rhythmic Features:\n",
    "\n",
    "    Tempo: Estimates the speed of the music in beats per minute (BPM).<br>\n",
    "    Beat Histogram: Represents the distribution of detected beat intervals, highlighting dominant rhythms.\n",
    "\n",
    "5. Harmonic Features:\n",
    "\n",
    "    Chroma Features: Capture the energy distribution across the 12 pitch classes, reflecting harmonic content.<br>\n",
    "    Tonnetz Features: Represent tonal relations in music, useful for analyzing harmonic structures.\n",
    "\n",
    "6. Mel-Frequency Cepstral Coefficients (MFCCs):\n",
    "\n",
    "    Coefficients that represent the short-term power spectrum of a sound, crucial for timbre analysis.\n",
    "\n",
    "For each feature except tempo, the mean and variance are given, leading to 57 features in total. \n",
    "\n",
    "These features are typically extracted using audio analysis libraries such as LibROSA, which provide tools for computing them from audio signals.\n",
    "\n",
    "The different columns of our dataset are displayed in the following, along with their respective data types. We can see that there are no null values among them, so we do not have to take that into consideration during the data preperation. Also, all of the 57 features are of numerical type, so we won't have to encode them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/features_30_sec.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the first few instances in our dataset. Since the file names contain the respective track's genre and the track lengths are all equal, we will not use them as additional features for training or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have a look at the correlations between the different features, which can be seen in the following correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df.iloc[:, 2:-2].corr()\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(cor, square = True, xticklabels=True, yticklabels=True) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are quite a few feature combinations with high correlations, notably in the upper left corner, for example between the rolloff mean and the spectral centroid mean, as well as the lower left corner between some of the mfcc values. Although this could be seen as a reason for dimensionality reduction by leaving out some of the highly correlated features, due to the complexity and number of features there is no telling whether some of these features could be important in combination. Thus, eventhough we have seen these correlations, we will still use all of the dimensions for training our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "Because the dataset is already in a good condition for further analysis, we won't have much to do in terms of data preparation. First, we will encode the genre labels numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "LabelEnc = LabelEncoder()\n",
    "y = df['label']\n",
    "y = pd.DataFrame(LabelEnc.fit_transform(y))\n",
    "df['label_enc'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will separate the dataset in labels and features and scale the features linearly to a scale between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mms = MinMaxScaler()\n",
    "\n",
    "X = df.loc[:, 'chroma_stft_mean' : 'mfcc20_var']\n",
    "X_scaled_array_mms = scaler_mms.fit_transform(X)\n",
    "\n",
    "X_scaled_mms = pd.DataFrame(X_scaled_array_mms, columns=X.columns)\n",
    "\n",
    "print(X)\n",
    "print(X_scaled_mms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separate the data we scaled into training and testing data, which already concludes the data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "X_train_mms, X_test_mms, y_train_mms, y_test_mms = train_test_split(X_scaled_mms, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training \n",
    "\n",
    "Each Modell is trained and the quality of the classifier(accuracy) is displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Forests\n",
    "Author: Spyridon S. \n",
    "\n",
    "In this section Random Forest as a classifier will be tested. In the first step all important libraries will be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training random forests, there is no heavy hyperparameter needed to get a good accuracy. The structure of the model is already decreasing Bias and Variance by injecting randomness on constructing the trees. By Random Feature selection and Bagging the Risk of Overfitting is minimized, and by slightly tuning the hyperparameters, the Underfitting risk is also minimized. Nevertheless, we will perform a soft hyperparameter tuning, as well as a heavy hyperparameter tuning. The aim is to see if it makes sense to preform a heavy hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Simple Hyperparameter tuning \n",
    "It is enough only to tune the numbers of trees in the ensemble \"n_estimators\" and the splitting criterion. All other hyperparameters will use the default values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "param_grid = {'n_estimators': np.array([ 100, 250, 500, 1000, 2000, 7000]), \n",
    "              'criterion':['gini','entropy', 'log_loss'],\n",
    "              }\n",
    "grid_search_rf_simple = GridSearchCV(rf, param_grid, n_jobs=-1, cv=2, scoring='accuracy', verbose=1, refit=True)\n",
    "start = dt.datetime.now()\n",
    "grid_search_rf_simple.fit(X_train_mms, y_train_mms.values.ravel()) # MinMaxScaler is not really needed for Random Forest, but we are using it to be consistent with other models\n",
    "end = dt.datetime.now()\n",
    "print(f\"Training for MinMaxScaler data took {end - start}\")\n",
    "y_pred_rf_simple = grid_search_rf_simple.predict(X_test_mms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch found out the best model, the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf_simple.score(X_test_mms, y_test_mms)\n",
    "print(\"Best Score: %f\" % grid_search_rf_simple.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", grid_search_rf_simple.best_params_)\n",
    "print(\"Optimal Model: \", grid_search_rf_simple.best_estimator_)\n",
    "accuracy_rf_simple = accuracy_score(y_test_mms, y_pred_rf_simple)\n",
    "print(f\"Accuracy: {accuracy_rf_simple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in ~20 seconds (on my machine), Gridsearch found a model with 76% accuracy. That's a really good result! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Heavy Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's also try some heavy hyperparameter tuning to see what results can be achieved: (This takes some time.... ~20min.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_heavy = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "param_grid_rf_heavy = {'n_estimators': np.array([ 100, 250, 500, 1000, 2000, 7000]), \n",
    "              'criterion':['gini','entropy', 'log_loss'],\n",
    "              'max_depth': np.array([3,5, 7,10, None]),\n",
    "                'min_samples_split': np.array([2, 5, 10]),\n",
    "                'min_samples_leaf': np.array([1, 2, 4]),\n",
    "                'max_features': np.array(['sqrt', 'log2'])\n",
    "              }\n",
    "grid_search_rf_heavy = GridSearchCV(rf_heavy, param_grid_rf_heavy, n_jobs=-1, cv=2, scoring='accuracy', verbose=1, refit=True)\n",
    "start = dt.datetime.now()\n",
    "grid_search_rf_heavy.fit(X_train_mms, y_train_mms.values.ravel())\n",
    "end = dt.datetime.now()\n",
    "print(f\"Training for MinMaxScaler data took {end - start}\")\n",
    "y_pred_heavy = grid_search_rf_heavy.predict(X_test_mms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that needed a lot of time. It took nearly 20 minutes to get the results. Hopefully the created model is better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf_heavy.score(X_test, y_test)\n",
    "print(\"Best Score: %f\" % grid_search_rf_heavy.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", grid_search_rf_heavy.best_params_)\n",
    "print(\"Optimal Model: \", grid_search_rf_heavy.best_estimator_)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_heavy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not... The score is greater, but the accuracy is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Best Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with heavy hyperparameter tuning, that needed more than 15 minutes, the resulting forest is not really giving much more performance. There might be a slightly better score of the found model, but the resulting accuracy of the model is worse. So our best Random Forest model is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_simple_best = grid_search_rf_simple.best_estimator_\n",
    "rf_simple_best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following the feature importance of the best random forest can be seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_simple_best = grid_search_rf_simple.best_estimator_\n",
    "\n",
    "importances = rf_simple_best.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns\n",
    "\n",
    "\n",
    "top_n = 15\n",
    "top_indices = indices[:top_n]\n",
    "top_importances = importances[top_indices]\n",
    "\n",
    "top_feature_names = [feature_names[idx] for idx in top_indices]\n",
    "print(\"Feature ranking with names:\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f\"Top {top_n} Feature Importances\")\n",
    "plt.bar(range(top_n), top_importances, align=\"center\")\n",
    "plt.xticks(range(top_n), top_feature_names, rotation=45, ha='right')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.xlim([-1, top_n])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Decision trees\n",
    "\n",
    "Author: Jan W.\n",
    "\n",
    "Now we will build single decision trees to see how big the difference is compared to the Random Forest model. Realistically, our expectations are not very high because the Random Forest model usually performs better than single trees.\n",
    "\n",
    "First we will build a full tree and see how it performs. Since the tree is not pruned, we expect some overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn import tree\n",
    "\n",
    "cls = DecisionTreeClassifier(random_state=random_state)\n",
    "full_tree = cls.fit(X_train_mms, y_train_mms)\n",
    "pred = full_tree.predict(X_test_mms)\n",
    "print('Test accuracy',accuracy_score(y_test_mms, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our expectations were not that high, an accuracy just over 50% is still pretty disappointing. Now lets see how a pruned tree performs instead.\n",
    "\n",
    "First we will build a tree using post-pruning. Post-pruning is done using hyperparameter-tuning with GridsearchCV and will be tried out first since it showed the most promise in exercises during the lecture. For that, we will draw a graph in which we can see all of the regularization parameter values (effective alpha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=random_state) #maybe use variable for random state so that all classifiers can be adjusted at the same time\n",
    "path = clf.cost_complexity_pruning_path(X_train_mms, y_train_mms)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use GridSearchCV to hypertune the alpha parameter, determining the best of the above shown values by using cross validation. Then, we will use the best resulting tree to predict the labels corresponding to our test set and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'ccp_alpha':ccp_alphas[:-1].tolist()}\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=random_state), parameters, cv=10, refit=True)\n",
    "gs.fit(X_train_mms,y_train_mms)\n",
    "tree_best = gs.best_estimator_\n",
    "pred = tree_best.predict(X_test_mms)\n",
    "print('Accuracy', accuracy_score(y_test_mms, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the hyperparamter tuning took quite some time, the post-pruned tree performs worse than the un-pruned tree, which is surprising. To analyze the decision tree, we will visualize its rules in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rules = export_text(tree_best, feature_names=X.columns)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "text = tree.plot_tree(tree_best, \n",
    "                   feature_names=X.columns.to_list(), \n",
    "                   filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we will list the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importance:\\n\")\n",
    "feature_importance = {}\n",
    "i = 0\n",
    "for col in X.columns:\n",
    "    feature_importance[col] = tree_best.feature_importances_[i]\n",
    "    i += 1\n",
    "features_sorted = sorted(feature_importance.items(), key=lambda x : x[1])\n",
    "features_sorted.reverse()\n",
    "for feature in features_sorted:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try pre-pruning with a lower maximum height of tree to avoid overfitting and use a cross validation again to hypertune the parameters, which in this case are the maximum height and the type of purity measurement (gini or entropy). First we build the tree using GridSearchCV again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "params = {'max_depth':np.arange(3,15),\n",
    "#          'min_samples_leaf':[3,5,10,15,20],\n",
    "#          'min_samples_split':[8,10,12,18,20,16],\n",
    "          'criterion':['gini','entropy']}\n",
    "gs = GridSearchCV(cls, params, scoring='accuracy', cv=10, verbose=3, n_jobs=-1)\n",
    "gs.fit(X_train_mms, y_train_mms)\n",
    "params_optimal = gs.best_params_\n",
    "\n",
    "print(\"Best Score: %f\" % gs.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see that the depth of this tree is quite a lot smaller than the one we got from post-pruning, although the training score still doesn't look very promising. Also, we will note that the tuning went considerably faster than that of the post-pruned tree. Now let's look at the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_best = DecisionTreeClassifier(random_state=random_state, criterion='entropy', max_depth=11) #, min_samples_leaf=20, min_samples_split=8)\n",
    "tree_best.fit(X_train_mms, y_train_mms)\n",
    "pred = tree_best.predict(X_test_mms)\n",
    "accuracy_dt = accuracy_score(y_test_mms, pred)\n",
    "print('Test accuracy',accuracy_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result we got on the test set is still not good at all, but at least we got over 50% this time, which makes this tree favorable compared to the post-pruning tree earlier. In the following cells we will again visualize the tree's rules and the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "text = tree.plot_tree(tree_best, \n",
    "                   feature_names=X.columns.to_list(), \n",
    "                   filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importance:\\n\")\n",
    "feature_importance = {}\n",
    "i = 0\n",
    "for col in X.columns:\n",
    "    feature_importance[col] = tree_best.feature_importances_[i]\n",
    "    i += 1\n",
    "features_sorted = sorted(feature_importance.items(), key=lambda x : x[1])\n",
    "features_sorted.reverse()\n",
    "for feature in features_sorted:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 KNN\n",
    "\n",
    "Author: Lisa K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the music genres are classified with the **k-Nearest Neighbors** algorithm. To enhance model performance, it can be useful to tune the following three hyperparameters via cross validation:\n",
    "* ***n_neighbors***  :  number of neighbors $k$\n",
    "* ***weights***  :  weights assigned to the nearest neighbors, especially relevant in case of ties\n",
    "  - 'uniform'  :  all neighbors have equal weights\n",
    "  - 'distance'  :  neighbors closer to the target point have higher weights\n",
    "* ***metric***  :  method for distance computation\n",
    "  - 'euclidean'  :  Euclidean distance\n",
    "  - 'manhatten'  :  Manhatten distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define the possible values for each of these parameters in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter combinations for hyperparameter tuning via cross validation \n",
    "params = {'n_neighbors': np.arange(1,20),               # parameter 'k' \n",
    "              'weights': ['uniform', 'distance'],       # parameter 'weights'\n",
    "              'metric' : ['euclidean','manhattan']}     # parameter 'metric'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then hyperparameter tuning is performed with the help of *GridSearchCV*, using 10-fold cross validation and accuracy as evaluation measure. The model is trained on the training data which have been normalized with the *MinMaxScaler*. Since kNN relies on distance measures, feature scaling is a necessary pre-processing step in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "# Use GridSearchCV to tune the chosen parameters\n",
    "gs = GridSearchCV(knn, params, scoring='accuracy', cv=10, verbose=0, n_jobs=-1, refit=True)\n",
    "# Train\n",
    "gs.fit(X_train_mms, y_train_mms.values.ravel())    # Use training data scaled with MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the optimal parameter set, choosing $k=3$ nearest neighbors, distance-dependent weights and Manhattan distance turns out to be the best combination in this experiment. Yet, it must be noted that distance-related weights are also computed if there are no ties, which might lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_optimal = gs.best_params_\n",
    "\n",
    "print(\"Best score: %f\" % gs.best_score_)\n",
    "print(\"Optimal hyperparameters: \", params_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this optimal classifier is taken to predict the music genres in the corresponding test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose optimal classifier to predict\n",
    "knn_optimal = gs.best_estimator_\n",
    "y_pred_optimal = knn_optimal.predict(X_test_mms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation shows that the tuned kNN model performs with an accuracy of 74% on these test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for tuned KNN\n",
    "accuracy_knn = accuracy_score(y_test_mms, y_pred_optimal)\n",
    "print('Accuracy:', accuracy_knn)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Networks - ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: *Adrian Desiderato and Jan Birkert*\n",
    "\n",
    "#### Hyperparameter Tuning and Model Training\n",
    "\n",
    "To get the best possible result for an Artificial Neural Network, we want to tune our hyperparameters (the hidden layer sizes, the batch size, the activation function and the learning rate) beforehand. Since our dataset has only 1000 entries, let's attempt to use an approach similar to the one given in material notebook no. 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(randint.rvs(2, 200), randint.rvs(2, 200))],\n",
    "    'activation': ['tanh', 'relu', 'logistic'],\n",
    "    'alpha': [0.0001, 0.001, 0.05],\n",
    "    'learning_rate_init': uniform(0.001, 0.01),\n",
    "    'batch_size': randint(16, 128),\n",
    "    'learning_rate': ['constant', 'adaptive', 'invscaling']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best possible tuning, we will test with a combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='adam',\n",
    "                   max_iter=2000,\n",
    "                   random_state=random_state)\n",
    "\n",
    "clf_mms = RandomizedSearchCV(mlp, params, n_jobs=-1, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we prepared the data with the `MinMaxScaler`, let's use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "clf_mms.fit(X_train_mms, y_train_mms.values.ravel())\n",
    "end = dt.datetime.now()\n",
    "print(f\"Training for MinMaxScaler data took {end - start}\")\n",
    "\n",
    "\n",
    "clf_mms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_mms = clf_mms.predict(X_train_mms)\n",
    "print('Train accuracy with MinMaxScaler:', accuracy_score(pred_train_mms, y_train_mms))\n",
    "pred_test_mms = clf_mms.predict(X_test_mms)\n",
    "print('Test accuracy with MinMaxScaler:', accuracy_score(pred_test_mms, y_test_mms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After few tests, the best results were reached with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_best_params = {'activation': 'relu',\n",
    " 'alpha': 0.001,\n",
    " 'batch_size': 63,\n",
    " 'beta_1': 0.9,\n",
    " 'beta_2': 0.999,\n",
    " 'early_stopping': False,\n",
    " 'epsilon': 1e-08,\n",
    " 'hidden_layer_sizes': (70, 170),\n",
    " 'learning_rate': 'invscaling',\n",
    " 'learning_rate_init': 0.0014079058662580348,\n",
    " 'max_fun': 15000,\n",
    " 'max_iter': 2000,\n",
    " 'momentum': 0.9,\n",
    " 'n_iter_no_change': 10,\n",
    " 'nesterovs_momentum': True,\n",
    " 'power_t': 0.5,\n",
    " 'random_state': 0,\n",
    " 'shuffle': True,\n",
    " 'solver': 'adam',\n",
    " 'tol': 0.0001,\n",
    " 'validation_fraction': 0.1,\n",
    " 'verbose': False,\n",
    " 'warm_start': False}\n",
    "\n",
    "ann_best = MLPClassifier(**ann_best_params)\n",
    "ann_best.fit(X_train_mms, y_train_mms.values.ravel())\n",
    "y_pred_ann = ann_best.predict(X_test_mms)\n",
    "accuracy_ann = accuracy_score(y_test_mms, y_pred_ann)\n",
    "print('Accuracy:', accuracy_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data scaled with the `MinMaxScaler` yields the best result with a test accuracy of around 75%. Of course, as we used `randint` to randomize our hyperparameters for the number of neurons of the two hidden layers as well as the batch size, the accuracy will vary slightly each time the notebook is run. We also tested an ANN with a third hidden layer, which, however, did not improve the test accuracy for `MinMaxScaler` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the resulting best models will be compared. Let's import needed libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we create an array with the best models and their names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [knn_optimal, tree_best, rf_simple_best, ann_best]\n",
    "model_names = ['KNN', 'Decision Tree', 'Random Forest', 'ANN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Accuracy of the models will be showed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, name in zip(models, model_names): \n",
    "    y_pred = model.predict(X_test_mms)\n",
    "    accuracy = accuracy_score(y_test_mms, y_pred)\n",
    "    print(f\"Accuracy of {name}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that ANNs have the best accuracy, followed by RFs and KNN. Decision Trees performed poorly here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score of all models will be calculated. \n",
    "The F1-Scores can be interpreted as follows: \n",
    "\n",
    "![image](https://images.prismic.io/encord/3b4efcda-c027-46d6-b4e1-02cad9ff5f48_image18.png?auto=compress,format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score of each model\n",
    "for model, name in zip(models, model_names): \n",
    "    y_pred = model.predict(X_test_mms)\n",
    "    f1 = f1_score(y_test_mms, y_pred, average='weighted')\n",
    "    print(f\"F1 Score of {name}: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So depending on the table in KNN, RF and ANN had a good result. The result of Decision Trees was OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ROC, AUC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good comparison also can be made using the ROC-Curve of each model and the AUC. Due to the fact that ROC is designed for binary classifiers and our dataset contains 10 classes, we could create a ROC curve for each tuple of classes. But we decided to use a technique that averages the ROC-curves of all classes so that one ROC curve can be calculated for a multi-class classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to binarize the labels for the ROC curve\n",
    "y_test_bin = label_binarize(y_test_mms, classes=np.unique(y))\n",
    "y_train_bin = label_binarize(y_train_mms, classes=np.unique(y))\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "classifiers = models\n",
    "labels = model_names\n",
    "\n",
    "for clf, label, clr, ls in zip(classifiers, labels, colors, linestyles):\n",
    "    classifier = OneVsRestClassifier(clf, n_jobs=-1)\n",
    "    y_score = classifier.fit(X_train_mms, y_train_bin).predict_proba(X_test_mms)    \n",
    "    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=clr, linestyle=ls, label='%s (AUC = %0.2f)' % (label, roc_auc))\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=2)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the ROC-graph, random forests and artificial neural networks have the best AUC. KNN is on third place and decision trees are last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "model_entries = []\n",
    "model_list = [\n",
    "    (\"KNN\", knn_optimal),\n",
    "    (\"Decision Tree\", tree_best),\n",
    "    (\"Random Forest\", rf_simple_best),\n",
    "    (\"MLP\", ann_best)\n",
    "]\n",
    "\n",
    "for model_name, model_obj in model_list:\n",
    "    y_pred = model_obj.predict(X_test_mms)\n",
    "    acc = accuracy_score(y_test_mms, y_pred)\n",
    "    f1 = f1_score(y_test_mms, y_pred, average='weighted')\n",
    "    classifier_ovr = OneVsRestClassifier(model_obj, n_jobs=-1)\n",
    "    y_score = classifier_ovr.fit(X_train_mms, y_train_bin).predict_proba(X_test_mms)\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_score.ravel())\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "    model_entries.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1 Score (weighted)\": f1,\n",
    "        \"ROC AUC (micro)\": roc_auc_val\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(model_entries)\n",
    "comparison_df.style.format(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test own songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Spyridon S.\n",
    "\n",
    "To test our own songs, we need to extract the features of our song. To do this we are going to use librosa, a library for sound analyzing. If not installed, please install it using ```pip install librosa```. We start by importing needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# pip install librosa , if you haven't already\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature extraction we use code from kaggle as a reference. (https://www.kaggle.com/code/esabellechen/gtzan-features-extract)\n",
    "\n",
    "The code is analyzing the sound files and extracting the needed features. Due to license restrictions, we will not upload the tested sound files. But the sound files were mostly taken from these websites: \n",
    "https://www.minimalstudio.de/download-wav/\n",
    "https://pixabay.com/music/search/wav/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To test it, proceed as follows:\n",
    "# 1. Create a folder called \"songs\" in the same directory as this script\n",
    "# 2. Create subfolders in \"songs\" for each genre\n",
    "# 3. Add songs to the respective genre folders\n",
    "# 4. Run the script\n",
    "# 5. The extracted features will be stored in the data_df variable\n",
    "\n",
    "# due to license restrictions, we cannot provide the songs used in the example\n",
    "\n",
    "\n",
    "data_df=[]\n",
    "songs_path = \"songs\"\n",
    "genres = os.listdir(songs_path)\n",
    "for g in genres:\n",
    "    print(f\"Processing {g}...\")\n",
    "    genre_path = os.path.join(songs_path, g)\n",
    "    for file_name in os.listdir(genre_path):\n",
    "        song_path = os.path.join(genre_path, file_name)\n",
    "        try:\n",
    "            Y, sr = librosa.load(song_path, mono=True)\n",
    "            chroma_stft = librosa.feature.chroma_stft(y=Y, sr=sr)\n",
    "            rms = librosa.feature.rms(y=Y)\n",
    "            spec_cent = librosa.feature.spectral_centroid(y=Y, sr=sr)\n",
    "            spec_bw = librosa.feature.spectral_bandwidth(y=Y, sr=sr)\n",
    "            rolloff = librosa.feature.spectral_rolloff(y=Y, sr=sr)\n",
    "            zcr = librosa.feature.zero_crossing_rate(Y)\n",
    "            harmony, perceptr = librosa.effects.harmonic(Y), librosa.effects.percussive(Y)\n",
    "            tempo, _ = librosa.beat.beat_track(y=Y, sr=sr)\n",
    "            mfcc = librosa.feature.mfcc(y=Y, sr=sr, n_mfcc=20)\n",
    "\n",
    "            features = [\n",
    "                f\"{file_name}\", len(Y),\n",
    "                np.mean(chroma_stft), np.var(chroma_stft),\n",
    "                np.mean(rms), np.var(rms),\n",
    "                np.mean(spec_cent), np.var(spec_cent),\n",
    "                np.mean(spec_bw), np.var(spec_bw),\n",
    "                np.mean(rolloff), np.var(rolloff),\n",
    "                np.mean(zcr), np.var(zcr),\n",
    "                np.mean(harmony), np.var(harmony),\n",
    "                np.mean(perceptr), np.var(perceptr),\n",
    "                float(tempo) \n",
    "                ]\n",
    "\n",
    "            for coeff in mfcc:\n",
    "                features.append(np.mean(coeff))\n",
    "                features.append(np.var(coeff))\n",
    "            \n",
    "            features.append(g)\n",
    "\n",
    "            data_df.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {song_path}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the features are extracted, let's put them on a dataframe and extract them to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['filename', 'length',\n",
    "         'chroma_stft_mean', 'chroma_stft_var',\n",
    "         'rms_mean', 'rms_var',\n",
    "         'spectral_centroid_mean', 'spectral_centroid_var',\n",
    "         'spectral_bandwidth_mean', 'spectral_bandwidth_var',\n",
    "         'rolloff_mean', 'rolloff_var',\n",
    "         'zero_crossing_rate_mean','zero_crossing_rate_var',\n",
    "         'harmony_mean', 'harmony_var',\n",
    "         'perceptr_mean', 'perceptr_var',\n",
    "         'tempo'] + \\\n",
    "         [f'mfcc{i+1}_{stat}' for i in range(20) for stat in ['mean', 'var']] + ['label']\n",
    "\n",
    "         \n",
    "print(len(columns))\n",
    "df_own = pd.DataFrame(data_df, columns=columns)\n",
    "#df_own=df_own.sort_values(by='filename')\n",
    "feature_columns = df_own.columns.difference(['filename', 'label', 'length'])\n",
    "\n",
    "df_own_scaled = df_own.copy()\n",
    "df_own_scaled[feature_columns] = scaler_mms.transform(df_own.loc[:, 'chroma_stft_mean' : 'mfcc20_var'])\n",
    "\n",
    "df_own.to_csv('genres30_features.csv',index=False)\n",
    "df_own_scaled.to_csv('genres30_features_scaled.csv',index=False)\n",
    "\n",
    "df_own_scaled.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifiers will predict the genres on these songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"True labels: \\n{df_own_scaled[['label' ,'filename']]}\")\n",
    "\n",
    "print(\"predicted labels: \")\n",
    "print(\"Random Forest: \")\n",
    "predicted_labels_rf = LabelEnc.inverse_transform(rf_simple_best.predict(df_own_scaled.loc[:, 'chroma_stft_mean':'mfcc20_var']))\n",
    "for i in range(len(predicted_labels_rf)):\n",
    "    print(f\"datapoint {i} -> True value: {df_own_scaled['label'][i]};  \\t \\t Prediction: {predicted_labels_rf[i]}\")\n",
    "print(\"\\nKNN: \")\n",
    "predicted_labels_knn = LabelEnc.inverse_transform(knn_optimal.predict(df_own_scaled.loc[:, 'chroma_stft_mean':'mfcc20_var']))\n",
    "for i in range(len(predicted_labels_knn)):\n",
    "    print(f\"datapoint {i} -> True value: {df_own_scaled['label'][i]};  \\t \\t Prediction: {predicted_labels_knn[i]}\")\n",
    "print(\"\\nDecision Tree: \")\n",
    "predicted_labels_dt = LabelEnc.inverse_transform(tree_best.predict(df_own_scaled.loc[:, 'chroma_stft_mean':'mfcc20_var']))\n",
    "for i in range(len(predicted_labels_dt)):\n",
    "    print(f\"datapoint {i} -> True value: {df_own_scaled['label'][i]};  \\t \\t Prediction: {predicted_labels_dt[i]}\")\n",
    "print(\"\\nANN: \")\n",
    "predicted_labels_ann = LabelEnc.inverse_transform(ann_best.predict(df_own_scaled.loc[:, 'chroma_stft_mean':'mfcc20_var']))\n",
    "for i in range(len(predicted_labels_ann)):\n",
    "    print(f\"datapoint {i} -> True value: {df_own_scaled['label'][i]};  \\t \\t Prediction: {predicted_labels_ann[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of misclassifications...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Summary\n",
    "#### 5.1.1 Data Preparation and Scaling  \n",
    "    \n",
    "The dataset contains no null or infinity values. Also, all data is numerical, so no encoding is needed. Only the label was encoded, and the features were scaled using MinMaxScaler. \n",
    "\n",
    "#### 5.1.2 Model Training and Hyperparameter Tuning  \n",
    "Each Model was trained on the same training dataset. On all models, hyperparameters were tuned so that better results can be achieved. For each model a small summary can be found in the following:\n",
    "- Random Forest (RF)  \n",
    "    - Used GridSearchCV for hyperparameter tuning, experimenting with different numbers of estimators and other parameters. \n",
    "    - The best model (rf_simple_best) was selected based on cross-validation accuracy.  \n",
    "    - Feature importances were visualized to understand which input variables contributed most.\n",
    "    - Also compared results of heavy/soft-hyperparameter-tuning and seen that soft-hyperparameter tuning resulted to better accuracy than heavy-hyperparameter tuning.   \n",
    "\n",
    "- Decision Tree (DT)  \n",
    "    - Demonstrated two techniques: post-pruning (via cost-complexity parameter ccp_alpha) and pre-pruning (via restricting max_depth).  \n",
    "    - Used GridSearchCV to optimize parameters such as ccp_alpha, max_depth, and splitting criteria (e.g., “gini” vs. “entropy”).  \n",
    "    - Visualized the resulting decision tree and extracted a text representation of its rules.  \n",
    "\n",
    "- k-Nearest Neighbors (kNN)  \n",
    "    - Tuned k (number of neighbors), weights (uniform or distance), and distance metric (e.g., Euclidean, Manhattan).  \n",
    "    - After 10-fold cross validation, the best combination of hyperparameters was chosen, and predictions were made on the test set.  \n",
    "\n",
    "- Artificial Neural Network (ANN, MLPClassifier)  \n",
    "    - Used RandomizedSearchCV to explore hidden layer sizes, activation functions, batch size, and learning rate.  \n",
    "    - Final chosen ANN model reached around 77% test accuracy.  \n",
    "\n",
    "#### 5.1.3 Model Evaluation and Comparison  \n",
    "The four main classifiers (KNN, Decision Tree, Random Forest, ANN) were compared on the test set and metrics included accuracy, F1 score (weighted), and ROC-AUC (micro-averaged for multi-class) were compared. A summary table was produced showing the performance of each best model.  \n",
    "\n",
    "#### 5.1.4 Experiment on own songs\n",
    "In our experiments we found out that all the models lack on performance when classifying own songs. The classifiers that had the most correct classifications were KNN and RF model with 2/14. ANN had 1/14 correct classifications and Decision trees had nothing correct. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Conclusion  \n",
    "Each of the four classifiers (KNN, Decision Tree, Random Forest, ANN) offered different trade-offs. There could be seen that decision trees seem to perform very poorly on this task, even with pruning the trees. Random Forest and ANN showed strong performance (accuracy around 77%), with kNN close behind. So for music classification, Random Forests, Artificial Neural Networks and K-Nearest-Neighbors can be used, but nearly $\\frac{1}{4}$ of the predictions theoretically would be false. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine Learning Lectures and Notebooks\n",
    "- https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\n",
    "- https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "- https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "- https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "- https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\n",
    "- https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- https://encord.com/blog/f1-score-in-machine-learning/#:~:text=Typically%2C%20an%20F1%20score%20%3E%200.9,to%20have%20a%20poor%20performance.\n",
    "- https://www.minimalstudio.de/download-wav/\n",
    "- https://pixabay.com/music/search/wav/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
